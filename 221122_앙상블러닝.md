# 20221122 멋쟁이사자처럼 AI 스쿨 7기
# K-MOOC 실습으로 배우는 머신러닝 7강

## TODO
1. 앙상블 러닝에 대한 정의
2. 앙상블 러닝 관련 용어

## DONE

### 앙상블 러닝 정의
여러 개의 단일 모델들의 평균치를 내거나, 투표를 해서 다수결에 의한 결정을 하는 등 여러 모델들의 집단 지성을 활용하여 더 나은 결과를 도출해 내는 것에 주 목적이 있습니다.

### 앙상블 기법 정의
* Voting (투표) - 투표를 통해 결과 도출
* Bagging - Bootstrap Aggregating (샘플을 다양하게 중복 생성)
* Boosting - 이전 오차를 보완하며 가중치 부여
* Stacking- 여러 모델을 기반으로 meta 모델

### Voting
Voting은 단어 뜻 그대로 투표를 통해 결정하는 방식입니다.

### Bagging
![image](https://user-images.githubusercontent.com/88615790/203268107-8cfa3e8b-c7d7-4ffe-bfe9-5f9468ecd92b.png)

Bagging은 Bootstrap Aggregating의 줄임말입니다.

Bootstrap = Sample(샘플)

Aggregating = 합산

이라고 풀이할 수 있겠습니다.

Bootstrap은 여러 개의 dataset을 중첩을 허용하게 하여 샘플링하여 분할하는 방식을 말합니다.

대표적인 Bagging 방식의 알고리즘인 RandomForest를 예를 들어 설명해 보겠습니다.

Decision Tree가 여러개 생성되게 되고, 각기 다른 dataset을 샘플링하게 되는데 중첩되는 dataset을 허용합니다.

데이터 셋의 구성이 [1, 2, 3, 4, 5 ]로 되어 있다면,

group 1 = [1, 2, 3]

group 2 = [1, 3, 4]

group 3 = [2, 3, 5]

로 구성된다고 말할 수 있겠습니다.

#### Bagging 앙상블의 장점
Bagging 기법을 활용하며 단일 model을 활용하여 prediction (예측)을 했을 때보다 variance를 줄이는 효과를 볼 수 있습니다

#### RandomForest

```c
# RandomForest
# n_jobs : CPU 코어의 수 -1 일 때는 전부 다 사용
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
```

* 주요 파라미터
    * n_estimators : 트리의 수
    * criterion: 가지의 분할의 품질을 측정하는 기능입니다.
    * max_depth: 트리의 최대 깊이입니다.
    * min_samples_split:내부 노드를 분할하는 데 필요한 최소 샘플 수입니다.
    * min_samples_leaf: 리프 노드에 있어야 하는 최소 샘플 수입니다.
    * max_leaf_nodes: 리프 노드 숫자의 제한치입니다.
    * random_state: 추정기의 무작위성을 제어합니다. 실행했을 때 같은 결과가 나오도록 합니다.

머신러닝 모델을 생성할 때 사용자가 직접 설정하는 값으로, 이를 어떻게 설정하느냐에 따라 모델의 성능이 달라집니다.

위의 모델은 트리의 수가 100개(n_estimators=100) 입니다.

##### 적절한 파라미터 찾는 법

##### 1) GridSearchCV()
    * 시도할 하이퍼파라미터들을 지정하면, 모든 조합에 대해 교차검증 후 가장 좋은 성능을 내는 하이퍼파라미터 조합을 찾음
      → GridSearchCV 에 목표로 하는 **하이퍼파라미터 별로 리스트**를 만들어 **딕셔너리 형태**로 파라미터 셋을 넣어주어 하이퍼파라미터와 값 범위를 지정해줄 수 있다.

###### GridSearchCV의 파라미터 설명
```c
class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, 
scoring=None, 
n_jobs=None, 
refit=True, 
cv=None, 
verbose=0, 
pre_dispatch='2*n_jobs', 
error_score=nan, 
return_train_score=False)
```
   * estimator: 모델객체 지정
   * param_grid: 하이퍼파라미터 정보담긴 딕셔너리
   * scoring: 평가 지표. 어떤 것을 기준으로 평가를 할 것인지 지정
   * n_jobs: Number of jobs to run in parallel. 사용할 CPU 코어 개수(-1 모든 코어 다 사용.)
   * refit: best parameter를 정할 때 사용하는 평가지표, scoring에 여러 평가지표를 설정한 경우
   * cv: 교차검증을 위한 fold 횟수 입니다. (교차검증 횟수)
   * verbose: iteration시마다 수행 결과 메시지를 출력
      - `verbose=0`(default): 메시지 출력 안함
      - `verbose=1`: the computation time for each fold 와 parameter candidate 출력
      - `verbose=2`: score 도 같이 출력
      - `verbose=3`: the fold and candidate parameter indexes, starting time of the computation 출력

###### GridSearchCV의 장단점
장점: 평가에 사용되는 데이터 편중을 막을 수 있다. 정확도를 향상시킬 수 있다.

단점: 여러번 학습하고 평가하는 과정을 거치기 때문에 모델 훈련/평가 시간이 오래걸린다.
##### 2) RandomizedSearchCV()
    * GridSearch 와 동일한 방식으로 사용하지만 모든 조합을 다 시도하지는 않고, 각 반복마다 임의의 값만 대입해 지정한 횟수만큼 평가

```c
# GridSearchCV => 지정된 조합만 보기 때문에 해당 그리드를 벗어나는 곳에 
# 좋은 성능을 내는 하이퍼파라미터가 있다면 찾지 못하는 단점이 있습니다.
# RandomizedSearchCV => 랜덤한 값을 넣고 하이퍼파라미터를 찾습니다. 
# 처음에는 범위를 넓게 지정하고 그 중에 좋은 성능을 내는 범위를 점점 좁혀가면서 찾습니다.
from sklearn.model_selection import RandomizedSearchCV

param_distributions = {"max_depth": np.random.randint(3, 100, 10), 
                       "max_features": np.random.uniform(0, 1, 10)}

clf = RandomizedSearchCV(estimator=model, 
                         param_distributions=param_distributions,
                         n_iter=5,
                         n_jobs=-1,
                         random_state=42,
                         verbose=2
                        )
clf.fit(X_train, y_train)
```

```c
# 위의 모델에 best_estimator_를 대입해 RandomForestClassifier 사용 시 어떤 패러미터가 가장 좋은 성능을 내는지를 알려줍니다.
clf.best_estimator_

# RandomForestClassifier(max_depth=10, max_features=0.42696895828972203,
                       n_jobs=-1, random_state=42)
```
max_depth (트리의 최대 깊이)는 10 입니다.

max_feature (The number of features to consider when looking for the best split)는 0.42696895828972203입니다.


```c
# 위의 best_estimator_로 찾은 패러미터들을 RandomForestClassifier 에 대입하여 예측한 데이터를 머신러닝 모델로 학습(fit)합니다.
# 데이터를 머신러닝 모델로 예측(predict)합니다.
best_model = clf.best_estimator_
y_predict = best_model.fit(X_train, y_train).predict(X_test)
```


래퍼런스:

https://teddylee777.github.io/machine-learning/ensemble%EA%B8%B0%EB%B2%95%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%A2%85%EB%A5%98-2
